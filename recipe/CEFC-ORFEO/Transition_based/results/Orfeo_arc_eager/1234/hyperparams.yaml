# Generated 2022-07-27 from:
# /home/pupiera/Nextcloud/thesis/ParseBrain/recipe/CEFC-ORFEO/Transition_based/hparams/transition_based.yaml
# yamllint disable
# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1234
__set_seed: !!python/object/apply:torch.manual_seed [1234]
output_folder: results/Orfeo_arc_eager/1234
wer_file: results/Orfeo_arc_eager/1234/wer.txt
save_folder: results/Orfeo_arc_eager/1234/save
train_log: results/Orfeo_arc_eager/1234/train_log.txt

#Data files
data_folder: /home/pupiera/getalp/pupiera/conllu/gold #/home/getalp/data/ASR_data/FR/CORPUS_AUDIO/cefc-orfeo_v.1.5_december2021/11/oral
train_conllu: /home/pupiera/getalp/pupiera/conllu/gold/orfeo_shuf.train
valid_conllu: /home/pupiera/getalp/pupiera/conllu/gold/orfeo_shuf.dev
test_conllu: /home/pupiera/getalp/pupiera/conllu/gold/orfeo_shuf.test

# Specific information related to conllu file
conllu_keys: [lineNumber, words, lemmas, POS, UPOS, tags, HEAD, DEP, tags2, tags3,
  timestamp_begin, timestamp_end, speaker]


# With data_parallel batch_size is split into N jobs
# With DDP batch_size is multiplied by N jobs
# Must be 6 per GPU to fit 16GB of VRAM
batch_size: 8
test_batch_size: 4

dataloader_options:
  batch_size: 8
  num_workers: 6
test_dataloader_options:
  batch_size: 4
  num_workers: 6



# Decoding parameters
# Be sure that the bos and eos index match with the BPEs ones
blank_index: 0
bos_index: 1
eos_index: 2


#tokenizer
pretrained_lm_path: /home/getalp/pupiera/thesis/endtoend_asr_multitask/src/LM/camembert-base
tokenizer: &id004 !new:sentencepiece.SentencePieceProcessor


# LM
lm_model: &id001 !new:speechbrain.lobes.models.transformer.TransformerLM.TransformerLM

  vocab: 32005 #Need to be the same as tranformer tokenizer (BPE). from tokenizer.vocab_size: 32005
  d_model: 768 #base size
  nhead: 12 # base
  num_encoder_layers: 12
  num_decoder_layers: 0
  d_ffn: 3072
  dropout: 0.0
  activation: !name:torch.nn.GELU
  normalize_before: false


# Training parameters
number_of_epochs: 30
lr: 1.0
ckpt_interval_minutes: 30 # save checkpoint every N minutes


#
# Functions and classes
#
epoch_counter: &id003 !new:speechbrain.utils.epoch_loop.EpochCounter

# pretrainer for pretrained LM
  limit: 30




modules:
  lm_model: *id001
model: &id002 !new:torch.nn.ModuleList
    #scheduler_model: !ref <lr_annealing_model>
- [*id001]
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: results/Orfeo_arc_eager/1234/save
  recoverables:
    model: *id002
    counter: *id003
pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
  collect_in: results/Orfeo_arc_eager/1234/save
  loadables:
    lm: *id001
    tokenizer: *id004
  paths:
    lm: /home/getalp/pupiera/thesis/endtoend_asr_multitask/src/LM/camembert-base/pytorch_model.bin # Camembert
    tokenizer: /home/getalp/pupiera/thesis/endtoend_asr_multitask/src/LM/camembert-base/sentencepiece.bpe.model # Camembert

